Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{bourke2009idome,
author = {Bourke, Paul},
journal = {CGAT09 Computer Games, Multimedia and Allied Technology},
pages = {265--272},
title = {{iDome: Immersive gaming with the Unity3D game engine}},
volume = {9},
year = {2009}
}
@article{sanberg2017free,
author = {Sanberg, Willem P and Dubbleman, Gijs and Others},
journal = {Electronic Imaging},
number = {19},
pages = {54--61},
publisher = {Society for Imaging Science and Technology},
title = {{Free-space detection with self-supervised and online trained fully convolutional networks}},
volume = {2017},
year = {2017}
}
@incollection{krizhevsky_imagenet_2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in {\{}Neural{\}} {\{}Information{\}} {\{}Processing{\}} {\{}Systems{\}} 25},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
pages = {1097--1105},
publisher = {Curran Associates, Inc.},
title = {{{\{}ImageNet{\}} {\{}Classification{\}} with {\{}Deep{\}} {\{}Convolutional{\}} {\{}Neural{\}} {\{}Networks{\}}}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@article{Paszke2017ENetAD,
author = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
journal = {CoRR},
title = {{ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation}},
volume = {abs/1606.0},
year = {2017}
}
@inproceedings{ciresan2012deep,
author = {Ciresan, Dan and Giusti, Alessandro and Gambardella, Luca M and Schmidhuber, J{\"{u}}rgen},
booktitle = {Advances in neural information processing systems},
pages = {2843--2851},
title = {{Deep neural networks segment neuronal membranes in electron microscopy images}},
year = {2012}
}
@article{JJoel,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.05519v1},
author = {Janai, Joel and Behl, Aseem and Geiger, Andreas},
eprint = {arXiv:1704.05519v1},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Computer Vision for Autonomous Vehicles Problems, Datasets, state of the art.pdf:pdf},
keywords = {and lowers the,autonomous vehicles,autonomous vision,by providing an exhaustive,computer vision,entry barrier for beginners,field of autonomous vision,for researchers in the,manner 1,overview,survey will become a,useful tool,we hope that our},
title = {{Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art}},
year = {2017}
}
@inproceedings{Cordts2016Cityscapes,
author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
booktitle = {Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{The Cityscapes Dataset for Semantic Urban Scene Understanding}},
year = {2016}
}
@article{Saez2018b,
abstract = {L-Arginine, the amino acid substrate for nitric oxide synthase, has been tested as a therapeutic intervention in a variety of chronic diseases and is commonly used as a nutritional supplement. In this study, we hypothesized that a subset of moderate to severe persistent asthma patients would benefit from supplementation with L-arginine by transiently increasing nitric oxide levels, resulting in bronchodilation and a reduction in inflammation. The pilot study consisted of a 3 month randomized, double-blind, placebo-controlled trial of L-arginine (0.05 g/kg twice daily) in patients with moderate to severe asthma. We measured spirometry, exhaled breath nitric oxide, serum arginine metabolites, questionnaire scores, daily medication use and PEFR with the primary endpoint being the number of minor exacerbations at three months. Interim analysis of the 20 subjects showed no difference in the number of exacerbations, exhaled nitric oxide levels or lung function between groups, though participants in the L-arginine group had higher serum L-arginine at day 60 (2.0 ± 0.6 × 10−3 vs. 1.1 ± 0.2 × 10−3 µmol/L, p {\textless} 0.05), ornithine at day 30 (2.4 ± 0.9 vs. 1.2 ± 0.3 µmol/L serum, p {\textless} 0.05) and ADMA at day 30 (6.0 ± 1.5 × 10−1 vs. 2.6 ± 0.6 × 10−1 µmol/L serum, p {\textless} 0.05) on average compared to the placebo group. The study was terminated prematurely. Supplementing asthma subjects with L-arginine increases plasma levels; whether subgroups might benefit from such supplementation requires further study.},
author = {S{\'{a}}ez, {\'{A}}lvaro and Bergasa, Luis M. and Romeral, Eduardo and L{\'{o}}pez, Elena and Barea, Rafael and Sanz, Rafael},
doi = {10.1109/IVS.2018.8500456},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/CNN-based Fisheye Image Real-Time Semantic Segmentation.pdf:pdf},
isbn = {9781538644522},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
pages = {1039--1044},
title = {{CNN-based Fisheye Image Real-Time Semantic Segmentation}},
volume = {2018-June},
year = {2018}
}
@inproceedings{long2015fully,
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {3431--3440},
title = {{Fully convolutional networks for semantic segmentation}},
year = {2015}
}
@inproceedings{alvarez2012road,
author = {Alvarez, Jose M and Gevers, Theo and LeCun, Yann and Lopez, Antonio M},
booktitle = {European Conference on Computer Vision},
organization = {Springer},
pages = {376--389},
title = {{Road scene segmentation from a single image}},
year = {2012}
}
@misc{craig2013inertia,
author = {Craig, Kevin C},
publisher = {CANON COMMUNICATIONS INC 11444 W OLYMPIC BLVD, SUITE 900, LOS ANGELES, CA{\~{}}�},
title = {{Inertia mismatch: fact or fiction?}},
year = {2013}
}
@article{Deng2018,
abstract = {Understanding the surrounding environment of the vehicle is still one of the challenges for autonomous driving. This paper addresses 360-degree road scene semantic segmentation using surround view cameras, which are widely equipped in existing production cars. First, in order to address large distortion problem in the fisheye images, Restricted Deformable Convolution (RDC) is proposed for semantic segmentation, which can effectively model geometric transformations by learning the shapes of convolutional filters conditioned on the input feature map. Second, in order to obtain a large-scale training set of surround view images, a novel method called zoom augmentation is proposed to transform conventional images to fisheye images. Finally, an RDC based semantic segmentation model is built. The model is trained for real-world surround view images through a multi-task learning architecture by combining real-world images with transformed images. Experiments demonstrate the effectiveness of the RDC to handle images with large distortions, and the proposed approach shows a good performance using surround view cameras with the help of the transformed images.},
archivePrefix = {arXiv},
arxivId = {1801.00708},
author = {Deng, Liuyuan and Yang, Ming and Li, Hao and Li, Tianyi and Hu, Bing and Wang, Chunxiang},
eprint = {1801.00708},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/Restricted Deformable Convolution based Road scene segmentation using surround view cameras.pdf:pdf},
pages = {1--10},
title = {{Restricted Deformable Convolution based Road Scene Semantic Segmentation Using Surround View Cameras}},
url = {http://arxiv.org/abs/1801.00708},
year = {2018}
}
@article{bolton2018customer,
author = {Bolton, Ruth N and McColl-Kennedy, Janet R and Cheung, Lilliemay and Gallan, Andrew and Orsingher, Chiara and Witell, Lars and Zaki, Mohamed},
journal = {Journal of Service Management},
number = {5},
pages = {776--808},
publisher = {Emerald Publishing Limited},
title = {{Customer experience challenges: bringing together digital, physical and social realms}},
volume = {29},
year = {2018}
}
@article{iten2018discovering,
author = {Iten, Raban and Metger, Tony and Wilming, Henrik and {Del Rio}, L$\backslash$'$\backslash$idia and Renner, Renato},
journal = {arXiv preprint arXiv:1807.10300},
title = {{Discovering physical concepts with neural networks}},
year = {2018}
}
@inproceedings{liu2008bird,
author = {Liu, Yu-Chih and Lin, Kai-Ying and Chen, Yong-Sheng},
booktitle = {International Workshop on Robot Vision},
organization = {Springer},
pages = {207--218},
title = {{Bird's-eye view vision system for vehicle surrounding monitoring}},
year = {2008}
}
@article{wang2014automatic,
author = {Wang, Chunxiang and Zhang, Hengrun and Yang, Ming and Wang, Xudong and Ye, Lei and Guo, Chunzhao},
journal = {Advances in Mechanical Engineering},
pages = {847406},
publisher = {SAGE Publications Sage UK: London, England},
title = {{Automatic parking based on a bird's eye view vision system}},
volume = {6},
year = {2014}
}
@incollection{boschert2016digital,
author = {Boschert, Stefan and Rosen, Roland},
booktitle = {Mechatronic Futures},
pages = {59--74},
publisher = {Springer},
title = {{Digital twin�the simulation aspect}},
year = {2016}
}
@inproceedings{deng2017cnn,
author = {Deng, Liuyuan and Yang, Ming and Qian, Yeqiang and Wang, Chunxiang and Wang, Bing},
booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
organization = {IEEE},
pages = {231--236},
title = {{CNN based semantic segmentation for urban traffic scenes using fisheye camera}},
year = {2017}
}
@inproceedings{boschert2018next,
author = {Boschert, Stefan and Rosen, R and Heinrich, C},
booktitle = {Proceedings of the 12th International Symposium on Tools and Methods of Competitive Engineering TMCE},
pages = {209--218},
title = {{Next generation digital Twin}},
year = {2018}
}
@article{article,
author = {Janai, Joel and Guney, Fatma and Behl, Aseem and Geiger, Andreas},
title = {{Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art}},
year = {2017}
}
@article{Ciresan2012,
abstract = {We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efﬁciently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artiﬁcial neural network as a pixel classiﬁer. The label of each pixel (membrane or nonmembrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classiﬁer is trained by plain gradient descent on a 512 512 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-speciﬁc postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.},
author = {Ciresan, Dc and Giusti, Alessandro and Gambardella, LM and Schmidhuber, J},
doi = {10.1016/S1673-8527(09)60051-5},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/nips2012.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Neural Information Processing Systems},
pages = {1--9},
pmid = {20513634},
title = {{Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images}},
url = {http://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images{\%}5Cnhttps://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf},
year = {2012}
}
@online{unity3d,
author = {Technologies, Unity},
title = {{Unity Editor}},
url = {https://unity3d.com/},
year = {2019}
}
@article{deng2018restricted,
author = {Deng, Liuyuan and Yang, Ming and Li, Hao and Li, Tianyi and Hu, Bing and Wang, Chunxiang},
journal = {arXiv preprint arXiv:1801.00708},
title = {{Restricted deformable convolution based road scene semantic segmentation using surround view cameras}},
year = {2018}
}
@article{banerjee2017generating,
author = {Banerjee, Agniva and Dalal, Raka and Mittal, Sudip and Joshi, Karuna Pande},
journal = {UMBC Information Systems Department},
publisher = {ACM Publications},
title = {{Generating digital twin models using knowledge graphs for industrial production lines}},
year = {2017}
}
@inproceedings{he2015delving,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE international conference on computer vision},
pages = {1026--1034},
title = {{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}},
year = {2015}
}
@article{Tsutsui2018,
abstract = {Identifying "free-space," or safely driveable regions in the scene ahead, is a fundamental task for autonomous navigation. While this task can be addressed using semantic segmentation, the manual labor involved in creating pixelwise annotations to train the segmentation model is very costly. Although weakly supervised segmentation addresses this issue, most methods are not designed for free-space. In this paper, we observe that homogeneous texture and location are two key characteristics of free-space, and develop a novel, practical framework for free-space segmentation with minimal human supervision. Our experiments show that our framework performs better than other weakly supervised methods while using less supervision. Our work demonstrates the potential for performing free-space segmentation without tedious and costly manual annotation, which will be important for adapting autonomous driving systems to different types of vehicles and environments},
archivePrefix = {arXiv},
arxivId = {arXiv:1711.05998v2},
author = {Tsutsui, Satoshi and Kerola, Tommi and Saito, Shunta and Crandall, David J.},
doi = {10.1109/CVPRW.2018.00145},
eprint = {arXiv:1711.05998v2},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/Minimizing Supervision for Free-space Segmentation.pdf:pdf},
isbn = {9781538661000},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {1101--1110},
title = {{Minimizing supervision for free-space segmentation}},
volume = {2018-June},
year = {2018}
}
@inproceedings{bearman2016s,
author = {Bearman, Amy and Russakovsky, Olga and Ferrari, Vittorio and Fei-Fei, Li},
booktitle = {European conference on computer vision},
organization = {Springer},
pages = {549--565},
title = {{What's the point: Semantic segmentation with point supervision}},
year = {2016}
}
@article{bourke2009idome,
author = {Bourke, Paul},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/unitypaper.pdf:pdf},
journal = {CGAT09 Computer Games, Multimedia and Allied Technology},
pages = {265--272},
title = {{iDome: Immersive gaming with the Unity3D game engine}},
volume = {9},
year = {2009}
}
@inproceedings{shotton2008semantic,
author = {Shotton, Jamie and Johnson, Matthew and Cipolla, Roberto},
booktitle = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
organization = {IEEE},
pages = {1--8},
title = {{Semantic texton forests for image categorization and segmentation}},
year = {2008}
}
@inproceedings{7471935,
author = {Eichenseer, A and Kaup, A},
booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2016.7471935},
issn = {2379-190X},
keywords = {image sequences;video surveillance;video surveilla},
month = {mar},
pages = {1541--1545},
title = {{A data set providing synthetic and real-world fisheye video sequences}},
year = {2016}
}
@article{Badino2009,
abstract = {Ambitious driver assistance for complex urban scenarios demands a complete awareness of the situation, including all moving and stationary objects that limit the free space. Recent progress in real-time dense stereo vision provides precise depth information for nearly every pixel of an image. This rises new questions: How can one efficiently analyze half a million disparity values of next generation imagers? And how can one find all relevant obstacles in this huge amount of data in real-time? In this paper we build a medium-level representation named "stixel-world". It takes into account that the free space in front of vehicles is limited by objects with almost vertical surfaces. These surfaces are approximated by adjacent rectangular sticks of a certain width and height. The stixel-world turns out to be a compact but flexible representation of the three-dimensional traffic situation that can be used as the common basis for the scene understanding tasks of driver assistance and autonomous systems.},
author = {Badino, Hern{\'{a}}n and Franke, Uwe and Pfeiffer, David},
doi = {10.1007/978-3-642-03798-6_6},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/The Stixel World - A Compact Medium Level.pdf:pdf},
isbn = {3642037976},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {51--60},
title = {{The stixel world - A compact medium level representation of the 3d-world}},
volume = {5748 LNCS},
year = {2009}
}
@inproceedings{saez2018cnn,
author = {S{\'{a}}ez, Alvaro and Bergasa, Luis M and Romeral, Eduardo and L{\'{o}}pez, Elena and Barea, Rafael and Sanz, Rafael},
booktitle = {2018 IEEE Intelligent Vehicles Symposium (IV)},
organization = {IEEE},
pages = {1039--1044},
title = {{Cnn-based fisheye image real-time semantic segmentation}},
year = {2018}
}
@article{madni2019leveraging,
author = {Madni, Azad M and Madni, Carla C and Lucero, Scott D},
journal = {Systems},
number = {1},
pages = {7},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Leveraging Digital Twin Technology in Model-Based Systems Engineering}},
volume = {7},
year = {2019}
}
@article{ioffe2015batch,
author = {Ioffe, Sergey and Szegedy, Christian},
journal = {arXiv preprint arXiv:1502.03167},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
year = {2015}
}
@article{8424832,
author = {{El Saddik}, A},
doi = {10.1109/MMUL.2018.023121167},
issn = {1070-986X},
journal = {IEEE MultiMedia},
keywords = {Haptic interfaces;Artificial intelligence;Market r},
month = {apr},
number = {2},
pages = {87--92},
title = {{Digital Twins: The Convergence of Multimedia Technologies}},
volume = {25},
year = {2018}
}
@article{pan_survey_2010,
abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
author = {Pan, S J and Yang, Q},
doi = {10.1109/TKDE.2009.191},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Data mining,Knowledge engineering,Knowledge transfer,Labeling,Learning systems,Machine learning,Machine learning algorithms,Space technology,Testing,Training data,Transfer learning,data mining,data mining.,inductive transfer learning,knowledge engineering,knowledge transfer,learning by example,machine learning,optimisation,survey,transductive transfer learning,unsupervised learning,unsupervised transfer learning},
month = {oct},
number = {10},
pages = {1345--1359},
title = {{A Survey on Transfer Learning}},
volume = {22},
year = {2010}
}
@inproceedings{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
isbn = {9783319245737},
issn = {16113349},
pmid = {23285570},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
year = {2015}
}
@article{yu2015multi,
author = {Yu, Fisher and Koltun, Vladlen},
journal = {arXiv preprint arXiv:1511.07122},
title = {{Multi-scale context aggregation by dilated convolutions}},
year = {2015}
}
@inproceedings{haltakov2012scene,
author = {Haltakov, Vladimir and Belzner, Heidrun and Ilic, Slobodan},
booktitle = {2012 IEEE Intelligent Vehicles Symposium},
organization = {IEEE},
pages = {105--110},
title = {{Scene understanding from a moving camera for object detection and free space estimation}},
year = {2012}
}
@article{Shotton2008,
abstract = {We propose semantic texton forests, efficient and powerful new low-level features. These are ensembles of decision trees that act directly on image pixels, and therefore do not need the expensive computation of filter-bank responses or local descriptors. They are extremely fast to both train and test, especially compared with k-means clustering and nearest-neighbor assignment of feature descriptors. The nodes in the trees provide (i) an implicit hierarchical clustering into semantic textons, and (ii) an explicit local classification estimate. Our second contribution, the bag of semantic textons, combines a histogram of semantic textons over an image region with a region prior category distribution. The bag of semantic textons is computed over the whole image for categorization, and over local rectangular regions for segmentation. Including both histogram and region prior allows our segmentation algorithm to exploit both textural and semantic context. Our third contribution is an image-level prior for segmentation that emphasizes those categories that the automatic categorization believes to be present. We evaluate on two datasets including the very challenging VOC 2007 segmentation dataset. Our results significantly advance the state-of-the-art in segmentation accuracy, and furthermore, our use of efficient decision forests gives at least a five-fold increase in execution speed.},
author = {Shotton, Jamie and Johnson, Matthew and Cipolla, Roberto},
doi = {10.1109/CVPR.2008.4587503},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/2008-CVPR-semantic-texton-forests.pdf:pdf},
isbn = {9781424422432},
journal = {26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
title = {{Semantic texton forests for image categorization and segmentation}},
year = {2008}
}
@article{Deng2017b,
abstract = {Semantic segmentation is an important step of visual scene understanding for autonomous driving. Recently, Convolutional Neural Network (CNN) based methods have successfully applied in semantic segmentation using narrow-angle or even wide-angle pinhole camera. However, in urban traffic environments, autonomous vehicles need wider field of view to perceive surrounding things and stuff, especially at intersections. This paper describes a CNN-based semantic segmentation solution using fisheye camera which covers a large field of view. To handle the complex scene in the fisheye image, Overlapping Pyramid Pooling (OPP) module is proposed to explore local, global and pyramid local region context information. Based on the OPP module, a network structure called OPP-net is proposed for semantic segmentation. The net is trained and evaluated on a fisheye image dataset for semantic segmentation which is generated from an existing dataset of urban traffic scenes. In addition, zoom augmentation, a novel data augmentation policy specially designed for fisheye image, is proposed to improve the net's generalization performance. Experiments demonstrate the outstanding performance of the OPP-net for urban traffic scenes and the effectiveness of the zoom augmentation.},
author = {Deng, Liuyuan and Yang, Ming and Qian, Yeqiang and Wang, Chunxiang and Wang, Bing},
doi = {10.1109/IVS.2017.7995725},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/CNN based Semantic Segmentation for Urban Traffic Scenes using fisheye camera.pdf:pdf},
isbn = {9781509048045},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
keywords = {Image, Radar, Lidar Signal Processing,Vehicle Environment Perception,Vision Sensing and Perception},
number = {Iv},
pages = {231--236},
title = {{CNN based semantic segmentation for urban traffic scenes using fisheye camera}},
year = {2017}
}
@book{awad2015efficient,
author = {Awad, Mariette and Khanna, Rahul},
publisher = {Apress},
title = {{Efficient learning machines: theories, concepts, and applications for engineers and system designers}},
year = {2015}
}
@article{brust2015convolutional,
author = {Brust, Clemens-Alexander and Sickert, Sven and Simon, Marcel and Rodner, Erik and Denzler, Joachim},
journal = {arXiv preprint arXiv:1502.06344},
title = {{Convolutional patch networks with spatial prior for road detection and urban scene understanding}},
year = {2015}
}
@inproceedings{tompson2015efficient,
author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {648--656},
title = {{Efficient object localization using convolutional networks}},
year = {2015}
}
@inproceedings{airsim2017fsr,
author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
booktitle = {Field and Service Robotics},
title = {{AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles}},
url = {https://arxiv.org/abs/1705.05065},
year = {2017}
}
@inproceedings{hanisch2017free,
author = {H{\"{a}}nisch, Simon and Evangelio, Rub{\'{e}}n Heras and Tadjine, Hadj Hamma and P{\"{a}}tzold, Michael},
booktitle = {2017 IEEE Intelligent Vehicles Symposium (IV)},
organization = {IEEE},
pages = {135--140},
title = {{Free-space detection with fish-eye cameras}},
year = {2017}
}
@inproceedings{badino2009stixel,
author = {Badino, Hern{\'{a}}n and Franke, Uwe and Pfeiffer, David},
booktitle = {Joint Pattern Recognition Symposium},
organization = {Springer},
pages = {51--60},
title = {{The stixel world-a compact medium level representation of the 3d-world}},
year = {2009}
}
@inproceedings{hane2015obstacle,
author = {H{\"{a}}ne, Christian and Sattler, Torsten and Pollefeys, Marc},
booktitle = {2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
organization = {IEEE},
pages = {5101--5108},
title = {{Obstacle detection for self-driving cars using only monocular cameras and wheel odometry}},
year = {2015}
}
@inproceedings{tsutsui2018minimizing,
author = {Tsutsui, Satoshi and Kerola, Tommi and Saito, Shunta and Crandall, David J},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
pages = {988--997},
title = {{Minimizing Supervision for Free-space Segmentation}},
year = {2018}
}
