Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{liang_real-time_2007,
abstract = {As use of in-vehicle information systems (IVISs) such as cell phones, navigation systems, and satellite radios has increased, driver distraction has become an important and growing safety concern. A promising way to overcome this problem is to detect driver distraction and adapt in-vehicle systems accordingly to mitigate such distractions. To realize this strategy, this paper applied support vector machines (SVMs), which is a data mining method, to develop a real-time approach for detecting cognitive distraction using drivers' eye movements and driving performance data. Data were collected in a simulator experiment in which ten participants interacted with an IVIS while driving. The data were used to train and test both SVM and logistic regression models, and three different model characteristics were investigated: how distraction was defined, which data were input to the model, and how the input data were summarized. The results show that the SVM models were able to detect driver distraction with an average accuracy of 81.1{\%}, outperforming more traditional logistic regression models. The best performing model (96.1{\%} accuracy) resulted when distraction was defined using experimental conditions (i.e., IVIS drive or baseline drive), the input data were comprised of eye movement and driving measures, and these data were summarized over a 40-s window with 95{\%} overlap of windows. These results demonstrate that eye movements and simple measures of driving performance can be used to detect driver distraction in real time. Potential applications of this paper include the design of adaptive in-vehicle systems and the evaluation of driver distraction},
author = {Liang, Y and Reyes, M L and Lee, J D},
doi = {10.1109/TITS.2007.895298},
issn = {1524-9050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
month = {jun},
number = {2},
pages = {340--350},
title = {{Real-{\{}Time{\}} {\{}Detection{\}} of {\{}Driver{\}} {\{}Cognitive{\}} {\{}Distraction{\}} {\{}Using{\}} {\{}Support{\}} {\{}Vector{\}} {\{}Machines{\}}}},
volume = {8},
year = {2007}
}
@misc{noauthor_survey_nodate,
title = {{A {\{}Survey{\}} on {\{}Transfer{\}} {\{}Learning{\}} - {\{}IEEE{\}} {\{}Journals{\}} {\&} {\{}Magazine{\}}}},
url = {https://ieeexplore.ieee.org/document/5288526/}
}
@incollection{Chen2018,
abstract = {Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter networks can capture sharper object boundaries by gradually recovering the spatial information. In this work, we propose to combine the advantages from both methods. Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries. We further explore the Xception model and apply the depthwise separable convolution to both Atrous Spatial Pyramid Pooling and decoder modules, resulting in a faster and stronger encoder-decoder network. We demonstrate the effectiveness of the proposed model on PASCAL VOC 2012 and Cityscapes datasets, achieving the test set performance of 89.0$\backslash${\%} and 82.1$\backslash${\%} without any post-processing. Our paper is accompanied with a publicly available reference implementation of the proposed models in Tensorflow at $\backslash$url{\{}https://github.com/tensorflow/models/tree/master/research/deeplab{\}}.},
archivePrefix = {arXiv},
arxivId = {1802.02611},
author = {Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-01234-2_49},
eprint = {1802.02611},
isbn = {9783030012335},
issn = {16113349},
keywords = {Depthwise separable convolution,Encoder-decoder,Semantic image segmentation,Spatial pyramid pooling},
pages = {833--851},
pmid = {9373016},
title = {{Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation}},
url = {http://link.springer.com/10.1007/978-3-030-01234-2{\_}49},
volume = {11211 LNCS},
year = {2018}
}
@misc{noauthor_head_nodate,
title = {{Head pose estimation in the wild using {\{}Convolutional{\}} {\{}Neural{\}} {\{}Networks{\}} and adaptive gradient methods - {\{}ScienceDirect{\}}}},
url = {https://www.sciencedirect.com/science/article/pii/S0031320317302327}
}
@inproceedings{tompson2015efficient,
author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {648--656},
title = {{Efficient object localization using convolutional networks}},
year = {2015}
}
@article{Ciresan2012,
abstract = {We address a central problem of neuroanatomy, namely, the automatic segmentation of neuronal structures depicted in stacks of electron microscopy (EM) images. This is necessary to efﬁciently map 3D brain structure and connectivity. To segment biological neuron membranes, we use a special type of deep artiﬁcial neural network as a pixel classiﬁer. The label of each pixel (membrane or nonmembrane) is predicted from raw pixel values in a square window centered on it. The input layer maps each window pixel to a neuron. It is followed by a succession of convolutional and max-pooling layers which preserve 2D information and extract features with increasing levels of abstraction. The output layer produces a calibrated probability for each class. The classiﬁer is trained by plain gradient descent on a 512 512 30 stack with known ground truth, and tested on a stack of the same size (ground truth unknown to the authors) by the organizers of the ISBI 2012 EM Segmentation Challenge. Even without problem-speciﬁc postprocessing, our approach outperforms competing techniques by a large margin in all three considered metrics, i.e. rand error, warping error and pixel error. For pixel error, our approach is the only one outperforming a second human observer.},
author = {Ciresan, Dc and Giusti, Alessandro and Gambardella, LM and Schmidhuber, J},
doi = {10.1016/S1673-8527(09)60051-5},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/nips2012.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Neural Information Processing Systems},
pages = {1--9},
pmid = {20513634},
title = {{Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images}},
url = {http://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images{\%}5Cnhttps://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf},
year = {2012}
}
@misc{noauthor_cs231n_nodate,
title = {{{\{}CS{\}}231n {\{}Convolutional{\}} {\{}Neural{\}} {\{}Networks{\}} for {\{}Visual{\}} {\{}Recognition{\}}}},
url = {http://cs231n.github.io/transfer-learning/}
}
@article{simonyan_very_2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
annote = {arXiv: 1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
journal = {arXiv:1409.1556 [cs]},
keywords = {Computer Science - Computer Vision and Pattern Rec},
month = {sep},
title = {{Very {\{}Deep{\}} {\{}Convolutional{\}} {\{}Networks{\}} for {\{}Large{\}}-{\{}Scale{\}} {\{}Image{\}} {\{}Recognition{\}}}},
url = {http://arxiv.org/abs/1409.1556},
year = {2014}
}
@article{mchugh_chi-square_2013,
abstract = {The Chi-square statistic is a non-parametric (distribution free) tool designed to analyze group differences when the dependent variable is measured at a nominal level. Like all non-parametric statistics, the Chi-square is robust with respect to the distribution of the data. Specifically, it does not require equality of variances among the study groups or homoscedasticity in the data. It permits evaluation of both dichotomous independent variables, and of multiple group studies. Unlike many other non-parametric and some parametric statistics, the calculations needed to compute the Chi-square provide considerable information about how each of the groups performed in the study. This richness of detail allows the researcher to understand the results and thus to derive more detailed information from this statistic than from many others., The Chi-square is a significance statistic, and should be followed with a strength statistic. The Cramer's V is the most common strength test used to test the data when a significant Chi-square result has been obtained. Advantages of the Chi-square include its robustness with respect to distribution of the data, its ease of computation, the detailed information that can be derived from the test, its use in studies for which parametric assumptions cannot be met, and its flexibility in handling data from both two group and multiple group studies. Limitations include its sample size requirements, difficulty of interpretation when there are large numbers of categories (20 or more) in the independent or dependent variables, and tendency of the Cramer's V to produce relative low correlation measures, even for highly significant results.},
author = {McHugh, Mary L},
doi = {10.11613/BM.2013.018},
issn = {1330-0962},
journal = {Biochemia Medica},
month = {jun},
number = {2},
pages = {143--149},
pmid = {23894860},
title = {{The {\{}Chi{\}}-square test of independence}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900058/},
volume = {23},
year = {2013}
}
@article{Shotton2008,
abstract = {We propose semantic texton forests, efficient and powerful new low-level features. These are ensembles of decision trees that act directly on image pixels, and therefore do not need the expensive computation of filter-bank responses or local descriptors. They are extremely fast to both train and test, especially compared with k-means clustering and nearest-neighbor assignment of feature descriptors. The nodes in the trees provide (i) an implicit hierarchical clustering into semantic textons, and (ii) an explicit local classification estimate. Our second contribution, the bag of semantic textons, combines a histogram of semantic textons over an image region with a region prior category distribution. The bag of semantic textons is computed over the whole image for categorization, and over local rectangular regions for segmentation. Including both histogram and region prior allows our segmentation algorithm to exploit both textural and semantic context. Our third contribution is an image-level prior for segmentation that emphasizes those categories that the automatic categorization believes to be present. We evaluate on two datasets including the very challenging VOC 2007 segmentation dataset. Our results significantly advance the state-of-the-art in segmentation accuracy, and furthermore, our use of efficient decision forests gives at least a five-fold increase in execution speed.},
author = {Shotton, Jamie and Johnson, Matthew and Cipolla, Roberto},
doi = {10.1109/CVPR.2008.4587503},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/2008-CVPR-semantic-texton-forests.pdf:pdf},
isbn = {9781424422432},
journal = {26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
title = {{Semantic texton forests for image categorization and segmentation}},
year = {2008}
}
@article{vora_driver_2018,
abstract = {Driver gaze has been shown to be an excellent surrogate for driver attention in intelligent vehicles. With the recent surge of highly autonomous vehicles, driver gaze can be useful for determining the handoff time to a human driver. While there has been significant improvement in personalized driver gaze zone estimation systems, a generalized system which is invariant to different subjects, perspectives and scales is still lacking. We take a step towards this generalized system using Convolutional Neural Networks (CNNs). We finetune 4 popular CNN architectures for this task, and provide extensive comparisons of their outputs. We additionally experiment with different input image patches, and also examine how image size affects performance. For training and testing the networks, we collect a large naturalistic driving dataset comprising of 11 long drives, driven by 10 subjects in two different cars. Our best performing model achieves an accuracy of 95.18{\%} during cross-subject testing, outperforming current state of the art techniques for this task. Finally, we evaluate our best performing model on the publicly available Columbia Gaze Dataset comprising of images from 56 subjects with varying head pose and gaze directions. Without any training, our model successfully encodes the different gaze directions on this diverse dataset, demonstrating good generalization capabilities.},
annote = {arXiv: 1802.02690},
author = {Vora, Sourabh and Rangesh, Akshay and Trivedi, Mohan M},
journal = {arXiv:1802.02690 [cs]},
keywords = {Computer Science - Computer Vision and Pattern Rec},
month = {feb},
shorttitle = {Driver {\{}Gaze{\}} {\{}Zone{\}} {\{}Estimation{\}} using {\{}Convoluti}},
title = {{Driver {\{}Gaze{\}} {\{}Zone{\}} {\{}Estimation{\}} using {\{}Convolutional{\}} {\{}Neural{\}} {\{}Networks{\}}: {\{}A{\}} {\{}General{\}} {\{}Framework{\}} and {\{}Ablative{\}} {\{}Analysis{\}}}},
url = {http://arxiv.org/abs/1802.02690},
year = {2018}
}
@misc{noauthor_cave_nodate,
title = {{{\{}CAVE{\}} {\{}$\backslash$textbar{\}} {\{}Database{\}}: {\{}Columbia{\}} {\{}Gaze{\}} {\{}Data{\}} {\{}Set{\}}}},
url = {http://www.cs.columbia.edu/CAVE/databases/columbia{\_}gaze/}
}
@article{Christian,
author = {Christian, H},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/Obstacle detection for self-driving using only monocular camera and wheel odometry.pdf:pdf},
title = {{Obstacle Detection for Self-Driving Cars Using Only Monocular Cameras and Wheel Odometry}}
}
@article{Sanberg2016,
abstract = {Recently, vision-based Advanced Driver Assist Systems have gained broad interest. In this work, we investigate free-space detection, for which we propose to employ a Fully Convolutional Network (FCN). We show that this FCN can be trained in a self-supervised manner and achieve similar results compared to training on manually annotated data, thereby reducing the need for large manually annotated training sets. To this end, our self-supervised training relies on a stereo-vision disparity system, to automatically generate (weak) training labels for the color-based FCN. Additionally, our self-supervised training facilitates online training of the FCN instead of offline. Consequently, given that the applied FCN is relatively small, the free-space analysis becomes highly adaptive to any traffic scene that the vehicle encounters. We have validated our algorithm using publicly available data and on a new challenging benchmark dataset that is released with this paper. Experiments show that the online training boosts performance with 5{\%} when compared to offline training, both for Fmax and AP.},
archivePrefix = {arXiv},
arxivId = {1604.02316},
author = {Sanberg, Willem P. and Dubbelman, Gijs and de With, Peter H. N.},
eprint = {1604.02316},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/Free-Space Detection with Self-Supervised and Online Trained.pdf:pdf},
title = {{Free-Space Detection with Self-Supervised and Online Trained Fully Convolutional Networks}},
url = {http://arxiv.org/abs/1604.02316},
year = {2016}
}
@article{shardlow_analysis_nodate,
abstract = {In this paper several feature selection methods are explored. These are analysed to see what eﬀect they have on the accuracy of a simple svm. Several ﬁlter and wrapper techniques are investigated. Hybrid methods which use combinations of ﬁlter and wrapper techniques are also investigated. Many ﬁlter methods are found to give no increase in accuracy for the classiﬁer. The most eﬀective method is found to be a hybrid method which is called ‘Ranked Forward Search'. This gives an increase in accuracy for the classiﬁer when using only a small subset of the possible features.},
author = {Shardlow, Matthew},
pages = {7},
title = {{An {\{}Analysis{\}} of {\{}Feature{\}} {\{}Selection{\}} {\{}Techniques{\}}}}
}
@article{Deng2018,
abstract = {Understanding the surrounding environment of the vehicle is still one of the challenges for autonomous driving. This paper addresses 360-degree road scene semantic segmentation using surround view cameras, which are widely equipped in existing production cars. First, in order to address large distortion problem in the fisheye images, Restricted Deformable Convolution (RDC) is proposed for semantic segmentation, which can effectively model geometric transformations by learning the shapes of convolutional filters conditioned on the input feature map. Second, in order to obtain a large-scale training set of surround view images, a novel method called zoom augmentation is proposed to transform conventional images to fisheye images. Finally, an RDC based semantic segmentation model is built. The model is trained for real-world surround view images through a multi-task learning architecture by combining real-world images with transformed images. Experiments demonstrate the effectiveness of the RDC to handle images with large distortions, and the proposed approach shows a good performance using surround view cameras with the help of the transformed images.},
archivePrefix = {arXiv},
arxivId = {1801.00708},
author = {Deng, Liuyuan and Yang, Ming and Li, Hao and Li, Tianyi and Hu, Bing and Wang, Chunxiang},
eprint = {1801.00708},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/Restricted Deformable Convolution based Road scene segmentation using surround view cameras.pdf:pdf},
pages = {1--10},
title = {{Restricted Deformable Convolution based Road Scene Semantic Segmentation Using Surround View Cameras}},
url = {http://arxiv.org/abs/1801.00708},
year = {2018}
}
@misc{noauthor_long_nodate,
title = {{Long {\{}Short{\}}-{\{}Term{\}} {\{}Memory{\}}}},
url = {https://dl.acm.org/citation.cfm?id=1246450}
}
@article{liang_how_2012,
abstract = {OBJECTIVE: In this study, the authors used algorithms to estimate driver distraction and predict crash and near-crash risk on the basis of driver glance behavior using the data set of the 100-Car Naturalistic Driving Study.
BACKGROUND: Driver distraction has been a leading cause of motor vehicle crashes, but the relationship between distractions and crash risk lacks detailed quantification.
METHOD: The authors compared 24 algorithms that varied according to how they incorporated three potential contributors to distraction--glance duration, glance history, and glance location--on how well the algorithms predicted crash risk.
RESULTS: Distraction estimated from driver eye-glance patterns was positively associated with crash risk. The algorithms incorporating ongoing off-road glance duration predicted crash risk better than did the algorithms incorporating glance history.Augmenting glance duration with other elements of glance behavior--1.5th power of duration and duration weighted by glance location--produced similar prediction performance as glance duration alone.
CONCLUSIONS: The distraction level estimated by the algorithms that include current glance duration provides the most sensitive indicator of crash risk.
APPLICATION: The results inform the design of algorithms to monitor driver state that support real-time distraction mitigation systems.},
author = {Liang, Yulan and Lee, John D and Yekhshatyan, Lora},
doi = {10.1177/0018720812446965},
issn = {0018-7208},
journal = {Human Factors},
month = {dec},
number = {6},
pages = {1104--1116},
pmid = {23397818},
shorttitle = {How dangerous is looking away from the road?},
title = {{How dangerous is looking away from the road? {\{}Algorithms{\}} predict crash risk from glance patterns in naturalistic driving}},
volume = {54},
year = {2012}
}
@article{Paszke2017ENetAD,
author = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
journal = {CoRR},
title = {{ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation}},
volume = {abs/1606.0},
year = {2017}
}
@misc{noauthor_transfer_2017,
abstract = {Deep learning models excel at learning from a large number of labeled examples, but typically do not generalize to conditions not seen during training. This post gives an overview of transfer learning, motivates why it warrants our application, and discusses practical applications and methods.},
booktitle = {Sebastian Ruder},
month = {mar},
title = {{Transfer {\{}Learning{\}} - {\{}Machine{\}} {\{}Learning{\}}'s {\{}Next{\}} {\{}Frontier{\}}}},
year = {2017}
}
@article{strayer_cell_2003,
abstract = {This research examined the effects of hands-free cell phone conversations on simulated driving. The authors found that these conversations impaired driver's reactions to vehicles braking in front of them. The authors assessed whether this impairment could be attributed to a withdrawal of attention from the visual scene, yielding a form of inattention blindness. Cell phone conversations impaired explicit recognition memory for roadside billboards. Eye-tracking data indicated that this was due to reduced attention to foveal information. This interpretation was bolstered by data showing that cell phone conversations impaired implicit perceptual memory for items presented at fixation. The data suggest that the impairment of driving performance produced by cell phone conversations is mediated, at least in part, by reduced attention to visual inputs.},
author = {Strayer, David L and Drews, Frank A and Johnston, William A},
issn = {1076-898X},
journal = {Journal of Experimental Psychology. Applied},
month = {mar},
number = {1},
pages = {23--32},
pmid = {12710835},
title = {{Cell phone-induced failures of visual attention during simulated driving}},
volume = {9},
year = {2003}
}
@misc{noauthor_impact_nodate,
abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
booktitle = {ResearchGate},
shorttitle = {The {\{}Impact{\}} of {\{}Driver{\}} {\{}Inattention{\}} on {\{}Near{\}}-{\{}}},
title = {{The {\{}Impact{\}} of {\{}Driver{\}} {\{}Inattention{\}} on {\{}Near{\}}-{\{}Crash{\}}/{\{}Crash{\}} {\{}Risk{\}}: {\{}An{\}} {\{}Analysis{\}} {\{}Using{\}} the 100-{\{}Car{\}} {\{}Naturalistic{\}} {\{}Driving{\}} {\{}Study{\}} {\{}Data{\}}}},
url = {https://www.researchgate.net/publication/242182089{\_}The{\_}Impact{\_}of{\_}Driver{\_}Inattention{\_}on{\_}Near-CrashCrash{\_}Risk{\_}An{\_}Analysis{\_}Using{\_}the{\_}100-Car{\_}Naturalistic{\_}Driving{\_}Study{\_}Data}
}
@article{ioffe2015batch,
author = {Ioffe, Sergey and Szegedy, Christian},
journal = {arXiv preprint arXiv:1502.03167},
title = {{Batch normalization: Accelerating deep network training by reducing internal covariate shift}},
year = {2015}
}
@article{Saez2018a,
abstract = {L-Arginine, the amino acid substrate for nitric oxide synthase, has been tested as a therapeutic intervention in a variety of chronic diseases and is commonly used as a nutritional supplement. In this study, we hypothesized that a subset of moderate to severe persistent asthma patients would benefit from supplementation with L-arginine by transiently increasing nitric oxide levels, resulting in bronchodilation and a reduction in inflammation. The pilot study consisted of a 3 month randomized, double-blind, placebo-controlled trial of L-arginine (0.05 g/kg twice daily) in patients with moderate to severe asthma. We measured spirometry, exhaled breath nitric oxide, serum arginine metabolites, questionnaire scores, daily medication use and PEFR with the primary endpoint being the number of minor exacerbations at three months. Interim analysis of the 20 subjects showed no difference in the number of exacerbations, exhaled nitric oxide levels or lung function between groups, though participants in the L-arginine group had higher serum L-arginine at day 60 (2.0 ± 0.6 × 10−3 vs. 1.1 ± 0.2 × 10−3 µmol/L, p {\textless} 0.05), ornithine at day 30 (2.4 ± 0.9 vs. 1.2 ± 0.3 µmol/L serum, p {\textless} 0.05) and ADMA at day 30 (6.0 ± 1.5 × 10−1 vs. 2.6 ± 0.6 × 10−1 µmol/L serum, p {\textless} 0.05) on average compared to the placebo group. The study was terminated prematurely. Supplementing asthma subjects with L-arginine increases plasma levels; whether subgroups might benefit from such supplementation requires further study.},
author = {S{\'{a}}ez, {\'{A}}lvaro and Bergasa, Luis M. and Romeral, Eduardo and L{\'{o}}pez, Elena and Barea, Rafael and Sanz, Rafael},
doi = {10.1109/IVS.2018.8500456},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/CNN-based Fisheye Image Real-Time Semantic Segmentation.pdf:pdf},
isbn = {9781538644522},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
pages = {1039--1044},
title = {{CNN-based Fisheye Image Real-Time Semantic Segmentation}},
volume = {2018-June},
year = {2018}
}
@inproceedings{Ronneberger2015,
abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
archivePrefix = {arXiv},
arxivId = {1505.04597},
author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-24574-4_28},
eprint = {1505.04597},
isbn = {9783319245737},
issn = {16113349},
pmid = {23285570},
title = {{U-net: Convolutional networks for biomedical image segmentation}},
year = {2015}
}
@misc{noauthor_icg_nodate,
title = {{{\{}ICG{\}} - {\{}AFLW{\}}}},
url = {https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/}
}
@article{ruder_overview_2016,
author = {Ruder, Sebastian},
month = {sep},
title = {{An overview of gradient descent optimization algorithms}},
url = {https://arxiv.org/abs/1609.04747},
year = {2016}
}
@inproceedings{he2015delving,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE international conference on computer vision},
pages = {1026--1034},
title = {{Delving deep into rectifiers: Surpassing human-level performance on imagenet classification}},
year = {2015}
}
@misc{noauthor_cave_nodate-1,
title = {{{\{}CAVE{\}} {\{}$\backslash$textbar{\}} {\{}Database{\}}: {\{}Columbia{\}} {\{}Gaze{\}} {\{}Data{\}} {\{}Set{\}}}},
url = {http://www.cs.columbia.edu/CAVE/databases/columbia{\_}gaze/}
}
@article{pan_survey_2010,
abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
author = {Pan, S J and Yang, Q},
doi = {10.1109/TKDE.2009.191},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Data mining,Knowledge engineering,Knowledge transfer,Labeling,Learning systems,Machine learning,Machine learning algorithms,Space technology,Testing,Training data,Transfer learning,data mining,data mining.,inductive transfer learning,knowledge engineering,knowledge transfer,learning by example,machine learning,optimisation,survey,transductive transfer learning,unsupervised learning,unsupervised transfer learning},
month = {oct},
number = {10},
pages = {1345--1359},
title = {{A {\{}Survey{\}} on {\{}Transfer{\}} {\{}Learning{\}}}},
volume = {22},
year = {2010}
}
@article{Shelhamer2017,
abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves improved segmentation of PASCAL VOC (30{\%} relative improvement to 67.2{\%} mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.},
archivePrefix = {arXiv},
arxivId = {1411.4038v2},
author = {Shelhamer, Evan and Long, Jonathan and Darrell, Trevor},
doi = {10.1109/TPAMI.2016.2572683},
eprint = {1411.4038v2},
isbn = {9781467369640},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Convolutional Networks,Deep Learning,Semantic Segmentation,Transfer Learning},
pmid = {16190471},
title = {{Fully Convolutional Networks for Semantic Segmentation}},
year = {2017}
}
@article{Brust2015,
abstract = {Classifying single image patches is important in many different applications, such as road detection or scene understanding. In this paper, we present convolutional patch networks, which are convolutional networks learned to distinguish different image patches and which can be used for pixel-wise labeling. We also show how to incorporate spatial information of the patch as an input to the network, which allows for learning spatial priors for certain categories jointly with an appearance model. In particular, we focus on road detection and urban scene understanding, two application areas where we are able to achieve state-of-the-art results on the KITTI as well as on the LabelMeFacade dataset. Furthermore, our paper offers a guideline for people working in the area and desperately wandering through all the painstaking details that render training CNs on image patches extremely difficult.},
archivePrefix = {arXiv},
arxivId = {1502.06344},
author = {Brust, Clemens-Alexander and Sickert, Sven and Simon, Marcel and Rodner, Erik and Denzler, Joachim},
eprint = {1502.06344},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/Convolutional Patch Networks with Spatial Prior for Road Detection and.pdf:pdf},
title = {{Convolutional Patch Networks with Spatial Prior for Road Detection and Urban Scene Understanding}},
url = {http://arxiv.org/abs/1502.06344},
year = {2015}
}
@misc{noauthor_feature_nodate,
title = {{Feature {\{}Selection{\}} {\{}Using{\}} {\{}Information{\}} {\{}Gain{\}} for {\{}Improved{\}} {\{}Structural{\}}-{\{}Based{\}} {\{}Alert{\}} {\{}Correlation{\}}}},
url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0166017}
}
@misc{noauthor_uci_nodate,
title = {{{\{}UCI{\}} {\{}Machine{\}} {\{}Learning{\}} {\{}Repository{\}}: {\{}DrivFace{\}} {\{}Data{\}} {\{}Set{\}}}},
url = {https://archive.ics.uci.edu/ml/datasets/DrivFace}
}
@article{srivastava_dropout:_2014,
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
journal = {Journal of Machine Learning Research},
pages = {1929--1958},
shorttitle = {Dropout},
title = {{Dropout: {\{}A{\}} {\{}Simple{\}} {\{}Way{\}} to {\{}Prevent{\}} {\{}Neural{\}} {\{}Networks{\}} from {\{}Overfitting{\}}}},
url = {http://jmlr.org/papers/v15/srivastava14a.html},
volume = {15},
year = {2014}
}
@article{Deng2017,
abstract = {Semantic segmentation is an important step of visual scene understanding for autonomous driving. Recently, Convolutional Neural Network (CNN) based methods have successfully applied in semantic segmentation using narrow-angle or even wide-angle pinhole camera. However, in urban traffic environments, autonomous vehicles need wider field of view to perceive surrounding things and stuff, especially at intersections. This paper describes a CNN-based semantic segmentation solution using fisheye camera which covers a large field of view. To handle the complex scene in the fisheye image, Overlapping Pyramid Pooling (OPP) module is proposed to explore local, global and pyramid local region context information. Based on the OPP module, a network structure called OPP-net is proposed for semantic segmentation. The net is trained and evaluated on a fisheye image dataset for semantic segmentation which is generated from an existing dataset of urban traffic scenes. In addition, zoom augmentation, a novel data augmentation policy specially designed for fisheye image, is proposed to improve the net's generalization performance. Experiments demonstrate the outstanding performance of the OPP-net for urban traffic scenes and the effectiveness of the zoom augmentation.},
author = {Deng, Liuyuan and Yang, Ming and Qian, Yeqiang and Wang, Chunxiang and Wang, Bing},
doi = {10.1109/IVS.2017.7995725},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/CNN based Semantic Segmentation for Urban Traffic Scenes using fisheye camera.pdf:pdf},
isbn = {9781509048045},
journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
keywords = {Image, Radar, Lidar Signal Processing,Vehicle Environment Perception,Vision Sensing and Perception},
number = {Iv},
pages = {231--236},
title = {{CNN based semantic segmentation for urban traffic scenes using fisheye camera}},
year = {2017}
}
@misc{noauthor_[1507.04760]_nodate,
title = {{[1507.04760] {\{}Driver{\}} {\{}Gaze{\}} {\{}Region{\}} {\{}Estimation{\}} {\{}Without{\}} {\{}Using{\}} {\{}Eye{\}} {\{}Movement{\}}}},
url = {https://arxiv.org/abs/1507.04760}
}
@misc{noauthor_[1512.03385]_nodate,
title = {{[1512.03385] {\{}Deep{\}} {\{}Residual{\}} {\{}Learning{\}} for {\{}Image{\}} {\{}Recognition{\}}}},
url = {https://arxiv.org/abs/1512.03385}
}
@article{Tsutsui2018,
abstract = {Identifying "free-space," or safely driveable regions in the scene ahead, is a fundamental task for autonomous navigation. While this task can be addressed using semantic segmentation, the manual labor involved in creating pixelwise annotations to train the segmentation model is very costly. Although weakly supervised segmentation addresses this issue, most methods are not designed for free-space. In this paper, we observe that homogeneous texture and location are two key characteristics of free-space, and develop a novel, practical framework for free-space segmentation with minimal human supervision. Our experiments show that our framework performs better than other weakly supervised methods while using less supervision. Our work demonstrates the potential for performing free-space segmentation without tedious and costly manual annotation, which will be important for adapting autonomous driving systems to different types of vehicles and environments},
archivePrefix = {arXiv},
arxivId = {arXiv:1711.05998v2},
author = {Tsutsui, Satoshi and Kerola, Tommi and Saito, Shunta and Crandall, David J.},
doi = {10.1109/CVPRW.2018.00145},
eprint = {arXiv:1711.05998v2},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/Minimizing Supervision for Free-space Segmentation.pdf:pdf},
isbn = {9781538661000},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {1101--1110},
title = {{Minimizing supervision for free-space segmentation}},
volume = {2018-June},
year = {2018}
}
@article{Alvarez2012,
author = {Alvarez, Jose M and Gevers, Theo and Lecun, Yann and Lopez, Antonio M},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/Road Scene Segmentation from a Single Image.pdf:pdf},
isbn = {978-3-642-33785-7, 978-3-642-33786-4},
journal = {European Conference on Computer Vision (ECCV 2012)},
pages = {376--389},
title = {{LNCS 7578 - Road Scene Segmentation from a Single Image}},
volume = {7578},
year = {2012}
}
@misc{noauthor_driver_nodate,
title = {{On driver gaze estimation: {\{}Explorations{\}} and fusion of geometric and data driven approaches - {\{}IEEE{\}} {\{}Conference{\}} {\{}Publication{\}}}},
url = {https://ieeexplore.ieee.org/abstract/document/7795623/}
}
@incollection{krizhevsky_imagenet_2012,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in {\{}Neural{\}} {\{}Information{\}} {\{}Processing{\}} {\{}Systems{\}} 25},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
pages = {1097--1105},
publisher = {Curran Associates, Inc.},
title = {{{\{}ImageNet{\}} {\{}Classification{\}} with {\{}Deep{\}} {\{}Convolutional{\}} {\{}Neural{\}} {\{}Networks{\}}}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@article{Badino2009,
abstract = {Ambitious driver assistance for complex urban scenarios demands a complete awareness of the situation, including all moving and stationary objects that limit the free space. Recent progress in real-time dense stereo vision provides precise depth information for nearly every pixel of an image. This rises new questions: How can one efficiently analyze half a million disparity values of next generation imagers? And how can one find all relevant obstacles in this huge amount of data in real-time? In this paper we build a medium-level representation named "stixel-world". It takes into account that the free space in front of vehicles is limited by objects with almost vertical surfaces. These surfaces are approximated by adjacent rectangular sticks of a certain width and height. The stixel-world turns out to be a compact but flexible representation of the three-dimensional traffic situation that can be used as the common basis for the scene understanding tasks of driver assistance and autonomous systems.},
author = {Badino, Hern{\'{a}}n and Franke, Uwe and Pfeiffer, David},
doi = {10.1007/978-3-642-03798-6_6},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Read/The Stixel World - A Compact Medium Level.pdf:pdf},
isbn = {3642037976},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {51--60},
title = {{The stixel world - A compact medium level representation of the 3d-world}},
volume = {5748 LNCS},
year = {2009}
}
@article{krizhevsky_imagenet_2017,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make training faster, we used non-saturating neurons and a very efﬁcient GPU implementation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {10.1145/3065386},
issn = {00010782},
journal = {Communications of the ACM},
month = {may},
number = {6},
pages = {84--90},
title = {{{\{}ImageNet{\}} classification with deep convolutional neural networks}},
url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
volume = {60},
year = {2017}
}
@article{JJoel,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.05519v1},
author = {Janai, Joel and Behl, Aseem and Geiger, Andreas},
eprint = {arXiv:1704.05519v1},
file = {:C$\backslash$:/Users/z003zxuz/Documents/Research{\_}Project/Free space detection/Computer Vision for Autonomous Vehicles Problems, Datasets, state of the art.pdf:pdf},
keywords = {and lowers the,autonomous vehicles,autonomous vision,by providing an exhaustive,computer vision,entry barrier for beginners,field of autonomous vision,for researchers in the,manner 1,overview,survey will become a,useful tool,we hope that our},
title = {{Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art}},
year = {2017}
}
@article{lee_speech-based_2001,
abstract = {As computer applications for cars emerge, a speech-based interface offers an appealing alternative to the visually demanding direct manipulation interface. However, speech-based systems may pose cognitive demands that could undermine driving safety. This study used a car-following task to evaluate how a speech-based e-mail system affects drivers' response to the periodic braking of a lead vehicle. The study included 24 drivers between the ages of 18 and 24 years. A baseline condition with no e-mail system was compared with a simple and a complex e-mail system in both simple and complex driving environments. The results show a 30{\%} (310 ms) increase in reaction time when the speech-based system is used. Subjective workload ratings and probe questions also indicate that speech-based interaction introduces a significant cognitive load, which was highest for the complex e-mail system. These data show that a speech-based interface is not a panacea that eliminates the potential distraction of in-vehicle computers. Actual or potential applications of this research include design of in-vehicle information systems and evaluation of their contributions to driver distraction.},
author = {Lee, J D and Caven, B and Haake, S and Brown, T L},
doi = {10.1518/001872001775870340},
issn = {0018-7208},
journal = {Human Factors},
number = {4},
pages = {631--640},
pmid = {12002011},
shorttitle = {Speech-based interaction with in-vehicle computers},
title = {{Speech-based interaction with in-vehicle computers: the effect of speech-based e-mail on drivers' attention to the roadway}},
volume = {43},
year = {2001}
}
@incollection{awad_machine_2015,
abstract = {(ML) is a branch of artificial intelligence that systematically applies algorithms to synthesize the underlying relationships among data and information. For example, ML systems can be trained on automatic speech recognition systems (such as iPhone's Siri) to convert acoustic information in a sequence of speech data into semantic structure expressed in the form of a string of words.},
address = {Berkeley, CA},
author = {Awad, Mariette and Khanna, Rahul},
booktitle = {Efficient {\{}Learning{\}} {\{}Machines{\}}: {\{}Theories{\}}, {\{}Concepts{\}}, and {\{}Applications{\}} for {\{}Engineers{\}} and {\{}System{\}} {\{}Designers{\}}},
doi = {10.1007/978-1-4302-5990-9_1},
editor = {Awad, Mariette and Khanna, Rahul},
isbn = {978-1-4302-5990-9},
keywords = {Data Mining,Reinforcement Learning,Supervise Learning,Training Dataset,Unsupervised Learning},
pages = {1--18},
publisher = {Apress},
title = {{Machine {\{}Learning{\}}}},
url = {https://doi.org/10.1007/978-1-4302-5990-9{\_}1},
year = {2015}
}
@misc{noauthor_keras_nodate,
title = {{Keras {\{}Documentation{\}}}},
url = {https://keras.io/}
}
